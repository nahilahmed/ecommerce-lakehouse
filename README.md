# Ecommerce Lakehouse

An end-to-end lakehouse implementation for an ecommerce platform, demonstrating modern data architecture best practices using Databricks, Confluent Cloud, GitHub Actions, and Streamlit.

## ğŸ“‹ Overview

This project implements a complete data lakehouse solution for ecommerce operations, integrating real-time data streaming with analytics and business intelligence capabilities. The architecture is designed to be cost-effective, leveraging the Databricks free edition for data processing and analytics.

## ğŸ—ï¸ Architecture & Tech Stack

### Core Components

- **Databricks (Free Edition)**: Data lakehouse platform for ETL, data processing, and analytics
- **Confluent Cloud**: Real-time data streaming and event management
- **GitHub Actions**: CI/CD automation and orchestration
- **Streamlit**: Interactive dashboards and data visualization
- **Delta Lake**: ACID-compliant data storage format for reliability and performance

### Data Flow

```
Real-time Events (Ecommerce) 
    â†“
Confluent Cloud (Kafka Topics)
    â†“
Databricks (ETL/Processing)
    â†“
Delta Lake (Data Lakehouse)
    â†“
Streamlit (Dashboards & Analytics)
```

## âœ¨ Key Features

- **Real-time Data Ingestion**: Stream ecommerce events via Confluent Cloud
- **Scalable Processing**: Databricks for distributed data processing
- **Data Quality**: Built-in validation and error handling
- **Interactive Analytics**: Streamlit dashboards for business insights
- **Automated Workflows**: GitHub Actions for CI/CD and scheduled jobs
- **Cost-Optimized**: Leverages free tier services where possible

## ğŸš€ Getting Started

### Prerequisites

- Databricks account (Free Edition)
- Confluent Cloud account
- GitHub repository
- Python 3.8+
- Streamlit

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/nahilahmed/ecommerce-lakehouse.git
   cd ecommerce-lakehouse
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure Databricks**
   - Create a Databricks workspace (Free Edition)
   - Generate personal access token
   - Configure connection details in your environment

5. **Configure Confluent Cloud**
   - Create Kafka cluster and topics
   - Set up API keys and endpoints

## ğŸ“Š Dashboard

Run the Streamlit dashboard:

```bash
streamlit run app.py
```

## ğŸ”„ Automation

GitHub Actions workflows automate:
- Data ingestion from Confluent Cloud
- ETL jobs on Databricks
- Data quality checks
- Dashboard updates

## ğŸ“ Project Structure

```
ecommerce-lakehouse/
â”œâ”€â”€ README.md
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # GitHub Actions CI/CD pipelines
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ notebooks/          # Databricks notebooks for ETL
â”‚   â””â”€â”€ jobs/              # Job configurations
â”œâ”€â”€ confluent/
â”‚   â””â”€â”€ topics/            # Kafka topic configurations
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ app.py             # Dashboard application
â”œâ”€â”€ src/
â”‚   â””â”€â”€ processors/        # Data processing modules
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ .env.example          # Environment variables template
```

## ğŸ› ï¸ Development

### Running Tests

```bash
pytest tests/
```

### Making Changes

1. Create a feature branch
2. Implement changes
3. Run tests and validation
4. Submit pull request for review

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.