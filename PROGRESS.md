# ShopMetrics ‚Äî Progress Tracker
**Project Code:** SHOPMETRICS-DATA-001

---

## Recent Updates

### 2026-02-22 ‚Äî Confluent Kafka Connectivity Verified
- ‚úÖ Signed up for Confluent Cloud, created free Basic cluster
- ‚úÖ Created topic `clickstream-events` (3 partitions, 24-hr retention)
- ‚úÖ Created `src/utils/05_verify_kafka_connectivity.py` ‚Äî widget-based connectivity notebook
- ‚úÖ Debugged Free Edition constraints: `startingOffsets=latest` on empty topic, infinite trigger not supported, implicit checkpoint not supported
- ‚úÖ Both connectivity tests pass (batch schema check + AvailableNow streaming)
- ‚è≥ Store credentials in Databricks Secret Scope (CLI setup pending)
- üìã **Next:** Day 11 ‚Äî Clickstream producer (`data-generator/produce_clickstream.py`)

### 2026-02-20 ‚Äî Gold Layer Complete + Daily Sales Dashboard
- ‚úÖ Created Gold master orchestration notebook (`src/gold/ingest_gold_tables.py`) with watermark-based metadata-driven execution
- ‚úÖ Created `src/gold/table_notebooks/daily_sales_summary.py` (FR-006) ‚Äî date/category revenue aggregations
- ‚úÖ Created `src/gold/table_notebooks/customer_ltv.py` (FR-007) ‚Äî LTV segmentation (High/Medium/Low)
- ‚úÖ Created `src/gold/table_notebooks/product_performance.py` (FR-008) ‚Äî units sold, revenue, category rank
- ‚úÖ Fixed bugs in customer_ltv and product_performance notebooks
- ‚úÖ Created Databricks Lakeview dashboard: `dashboards/Shopcommerce - Daily Sales Visualization.lvdash.json`
- ‚úÖ Updated `src/utils/04_gold_metadata_setup.py` with all three Gold table registrations + dashboard config

### 2026-02-19 ‚Äî Silver Layer Review & Optimization
- ‚úÖ Reviewed unified silver ingestion script (`src/silver/ingest_silver_tables.py`)
- ‚úÖ Fixed critical bug: Main execution loop now respects `processing_order` from metadata
- ‚úÖ Fixed variable scope bug in `process_fact()` function
- ‚úÖ Enhanced dependency validation with detailed status feedback
- ‚úÖ Verified SCD Type 1, SCD Type 2, and Fact processing implementations

---

## Week 1 ‚Äî Foundation & Batch Data Setup

### Day 1 ‚Äî Databricks Workspace Setup
- [x] Sign up for Databricks Free Edition
- [x] Create Unity Catalog: catalog=shopmetrics_ecommerce, schemas=bronze/silver/gold
- [x] Create volume: ecommerce.bronze.raw_data
- [x] Verify Unity Catalog lineage tracking enabled

### Day 2 ‚Äî GitHub Repo Setup
- [x] Create GitHub repo with README, .gitignore
- [x] Create folder structure: src/{bronze,silver,gold,utils}/, tests/, data-generator/, docs/
- [x] Add BRD and implementation plan to docs/
- [x] Add CLAUDE.md for AI-assisted development context

### Day 3 ‚Äî Data Generators (Customers + Orders)
- [x] `data-generator/generate_customers.py` ‚Äî 10K customers, historical + incremental with attribute changes
- [x] `data-generator/generate_orders.py` ‚Äî 100K orders, historical + incremental with status transitions
- [x] Run generate_customers.py (historical) in Databricks
- [x] Run generate_orders.py (historical) in Databricks

### Day 4 ‚Äî Data Generator (Products) + Bronze Ingestion
- [x] `data-generator/generate_products.py` ‚Äî 1K products across 8 categories, historical + incremental with price changes
- [x] Run generate_products.py (historical) in Databricks
- [x] Upload/verify all CSVs in ecommerce.bronze.raw_data volume
- [x] Create src/bronze/ingest_orders.py ‚Äî add ingested_at, source_file audit columns
- [x] Write to ecommerce.bronze.orders_raw ‚Äî verify schema matches BRD ¬ß7.2
- [x] Test with SELECT, COUNT, DESCRIBE

### Day 5 ‚Äî Bronze Complete + First Silver
- [x] Ingest ecommerce.bronze.customers_raw and products_raw
- [x] Create metadata-driven framework (src/utils/03_silver_metadata_setup.py)
- [x] Create unified silver processing script (src/silver/ingest_silver_tables.py)
- [x] Implement orders_clean processing (fact table with dimension joins)
- [ ] Draft docs/data-model.md

### Day 6 ‚Äî SCD Type 2
- [x] Create SCD Type 2 implementation in src/silver/ingest_silver_tables.py (dim_customers)
- [x] Create SCD Type 1 implementation in src/silver/ingest_silver_tables.py (dim_products)
- [x] Implement dependency-ordered execution (processing_order column)
- [x] Add dynamic dimension joins for fact tables (SCD2 point-in-time support)
- [x] Review and fix execution ordering bugs
- [ ] Write tests/unit/test_scd2_customers.py
- [ ] Test AC-003: update customer email, verify old record end_date populated

---

## Week 2 ‚Äî Gold Layer + Confluent Kafka Setup

### Day 7 ‚Äî Daily Sales Summary
- [x] Create `src/gold/table_notebooks/daily_sales_summary.py` (FR-006)
- [ ] Verify AC-005: revenue within 0.01% of manual calc

### Day 8 ‚Äî Customer LTV
- [x] Create `src/gold/table_notebooks/customer_ltv.py` (FR-007)
- [ ] Verify AC-006: 3 test profiles segmented correctly

### Day 9 ‚Äî Product Performance
- [x] Create `src/gold/table_notebooks/product_performance.py` (FR-008)
- [ ] Full batch pipeline end-to-end run
- [ ] Verify NFR-001: pipeline completes within 60 minutes

### Day 10 ‚Äî Confluent Kafka Cluster
- [x] Sign up for Confluent Cloud, create free Basic cluster
- [x] Create topic: clickstream-events (3 partitions, 24-hr retention)
- [x] Verify connectivity from Databricks (`src/utils/05_verify_kafka_connectivity.py`)
- [ ] Store API key + secret in Databricks Secret Scope (via CLI)

### Day 11 ‚Äî Clickstream Producer
- [x] Create `data-generator/produce_clickstream.py` ‚Äî session-aware, ~1 event/sec
- [x] Test locally, verified messages flowing in Confluent Cloud UI

### Day 12 ‚Äî Streaming Bronze
- [ ] Store API key + secret in Databricks Secret Scope (via CLI)
- [x] Create `src/bronze/stream_clickstream.py` ‚Äî Structured Streaming from Kafka
- [ ] Run notebook, verify rows in `ecommerce.bronze.clickstream_raw`
- [ ] Verify sub-5-minute latency (NFR-002)

### Day 13 ‚Äî Streaming Silver + Gold
- [ ] Create src/silver/sessionize_clickstream.py (30-min window)
- [ ] Create src/gold/hourly_traffic_metrics.py (FR-009)
- [ ] Full streaming E2E: producer ‚Üí Kafka ‚Üí bronze ‚Üí silver ‚Üí gold

---

## Week 3 ‚Äî CI/CD, Testing & Asset Bundles

### Day 14 ‚Äî Secrets & PAT
- [ ] Generate Databricks PAT, add GitHub Secrets

### Day 15 ‚Äî Asset Bundles
- [ ] Create databricks.yml config (dev + prod targets)
- [ ] Validate locally: databricks bundle validate

### Day 16 ‚Äî Test Suite
- [ ] tests/unit/test_scd2_customers.py (AC-003)
- [ ] tests/unit/test_gold_sales.py (AC-005)
- [ ] tests/unit/test_data_quality.py (AC-004)
- [ ] tests/unit/test_kafka_producer.py

### Day 17 ‚Äî CI Pipeline
- [ ] .github/workflows/ci.yml (black, flake8, mypy, pytest)
- [ ] .pre-commit-config.yaml

### Day 18 ‚Äî CD Pipeline
- [ ] .github/workflows/cd.yml (deploy on merge to main)

### Day 19 ‚Äî Scheduling + DQ
- [ ] .github/workflows/schedule.yml (cron 02:00 UTC)
- [ ] src/utils/data_quality.py (FR-005)
- [ ] docs/deployment-guide.md

### Day 20 ‚Äî Optimisation
- [ ] OPTIMIZE + ZORDER on gold tables (FR-011)
- [ ] VACUUM with 7-day retention
- [ ] Unity Catalog table tags (FR-012)

---

## Week 4 ‚Äî Dashboard & Public Launch

### Day 21‚Äì27 ‚Äî Streamlit Dashboard
- [ ] dashboard/streamlit_app.py scaffold
- [ ] dashboard/utils/databricks_connector.py (with Parquet fallback)
- [ ] dashboard/pages/1_Sales_Performance.py (FR-013)
- [ ] dashboard/pages/2_Customer_Analytics.py (FR-014)
- [ ] dashboard/pages/3_Product_Insights.py (FR-015)
- [ ] dashboard/pages/4_Kafka_Traffic.py (FR-016)
- [ ] Deploy to Streamlit Community Cloud
- [ ] Verify AC-009, AC-010, AC-011

---

## Week 5 ‚Äî Documentation & Launch

### Day 28‚Äì30
- [ ] docs/deployment-guide.md (AC-013)
- [ ] README with architecture diagram + live dashboard link
- [ ] Verify AC-012: no credentials in repo

---

## Milestone Tracker

| Milestone | Target Day | Status |
|-----------|-----------|--------|
| Bronze layer complete | 5 | ‚úÖ Complete |
| Batch medallion done | 9 | ‚úÖ Complete (Gold notebooks built; AC verifications pending) |
| Kafka streaming live | 13 | Not started |
| CI/CD operational | 18 | Not started |
| All 4 dashboard pages live | 26 | üü° In Progress (Lakeview daily sales dashboard live) |
| All ACs signed off | 27 | Not started |
| Project publicly launched | 30 | Not started |
